{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1fb25-6959-4ef0-8b27-070da6e8f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports (ML + AI)\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "\n",
    "# --- Scikit-Learn (O que já tinhas) ---\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "# --- CORREÇÃO AQUI ---\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, RocCurveDisplay, f1_score\n",
    "\n",
    "# --- NOVAS: TensorFlow / Keras (IA) ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- Opções de Display ---\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"All libraries imported successfully (including TensorFlow/Keras and f1_score)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893125a3-7d11-491a-878c-e4306ef95189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load CSV and get a quick overview\n",
    "df = pd.read_csv('survey.csv')\n",
    "\n",
    "# Show the dimensions of the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Display the first 6 rows\n",
    "display(df.head(6))\n",
    "\n",
    "# Display summary info about the dataset (column types, non-null counts, etc.)\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef18e8-b8ee-4b54-b505-c7f92f80a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "# List all columns\n",
    "print(\"Columns in the dataset:\", list(df.columns))\n",
    "\n",
    "# Show counts of each value in the target column ('treatment')\n",
    "print(\"\\nTarget value counts (treatment):\")\n",
    "print(df['treatment'].value_counts(dropna=False))\n",
    "\n",
    "# Calculate percentage of missing values per column\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "print(\"\\nColumns with missing values (%):\")\n",
    "display(missing_pct[missing_pct > 0].round(2))\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(data=df, x='treatment', order=df['treatment'].value_counts().index)\n",
    "plt.title('Distribution of the target: treatment')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the distribution of age\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df['Age'].dropna(), bins=30)\n",
    "plt.title('Age distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5391d1-6000-4c1e-b109-eb33775bb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Basic cleaning: Age, Gender normalization, and mapping Yes/No to 1/0\n",
    "df2 = df.copy()  # work on a copy to preserve original data\n",
    "\n",
    "# Age cleaning: convert to numeric and filter out unrealistic values\n",
    "\n",
    "df2['Age'] = pd.to_numeric(df2['Age'], errors='coerce')  # convert to numeric, invalids become NaN\n",
    "print(\"Before filtering: number of null ages =\", df2['Age'].isna().sum())\n",
    "\n",
    "# Optional: remove ages outside a realistic range (14-100)\n",
    "df2.loc[(df2['Age'] < 14) | (df2['Age'] > 100), 'Age'] = np.nan\n",
    "print(\"After filtering: number of null ages =\", df2['Age'].isna().sum())\n",
    "\n",
    "\n",
    "# Gender normalization: map variations to Male / Female / Other\n",
    "\n",
    "def clean_gender(x):\n",
    "    if pd.isna(x):\n",
    "        return 'Other'\n",
    "    s = str(x).strip().lower()\n",
    "    # common male variants\n",
    "    if s in ['male', 'm', 'man', 'male-ish', 'maile', 'mal', 'cis male', 'male (cis)']:\n",
    "        return 'Male'\n",
    "    # common female variants\n",
    "    if s in ['female', 'f', 'woman', 'female (cis)', 'cis female']:\n",
    "        return 'Female'\n",
    "    # anything else (including 'trans' or 'non-binary') as Other\n",
    "    return 'Other'\n",
    "\n",
    "df2['Gender_clean'] = df2['Gender'].apply(clean_gender)\n",
    "\n",
    "\n",
    "# Map binary Yes/No columns to 1/0\n",
    "\n",
    "binary_cols = ['self_employed', 'family_history', 'treatment', 'remote_work', 'tech_company']\n",
    "\n",
    "# --- NOTA: Esta lógica é frágil e foi a causa dos nossos erros. ---\n",
    "# --- Vamos corrigi-la na Célula 7, como fizemos no outro notebook ---\n",
    "with warnings.catch_warnings(): # Silenciar os FutureWarnings que isto vai dar\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    for col in binary_cols:\n",
    "        if col in df2.columns:\n",
    "            df2[col] = df2[col].map({'Yes': 1, 'No': 0})  # convert Yes/No to 1/0\n",
    "            # keep existing numeric values as-is\n",
    "            df2[col] = df2[col].fillna(df2[col]).infer_objects(copy=False)\n",
    "\n",
    "print(\"Basic cleaning done. Sample data:\")\n",
    "display(df2[['Age','Gender','Gender_clean','self_employed','family_history','treatment']].head())\n",
    "\n",
    "# (Gráficos omitidos para ser breve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4fc1f-588f-41d3-9da6-d2756c81fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Initial features and simple new features\n",
    "df3 = df2.copy()  # work on a new copy to preserve previous cleaning\n",
    "\n",
    "# Example of a binary feature: long_hours (e.g., work hours > 50)\n",
    "# Note: if there is no 'hours' column in the dataset, we skip this part.\n",
    "# Here, we assume there is no 'hours' column, so we do not create it.\n",
    "\n",
    "# List of candidate features to consider for analysis/modeling\n",
    "candidate_features = [\n",
    "    'Age', \n",
    "    'Gender_clean',\n",
    "    'self_employed',\n",
    "    'family_history',\n",
    "    'work_interfere',   # categorical: 'Never','Rarely','Sometimes','Often'\n",
    "    'no_employees',     # categorical: company size\n",
    "    'remote_work',\n",
    "    'tech_company',\n",
    "    'benefits',\n",
    "    'care_options',\n",
    "    'wellness_program',\n",
    "    'seek_help',\n",
    "    'anonymity'\n",
    "]\n",
    "\n",
    "# Keep only the features that exist in the current dataframe\n",
    "candidate_features = [c for c in candidate_features if c in df3.columns]\n",
    "print(\"Candidate features used:\", candidate_features)\n",
    "\n",
    "# Quick peek at unique values for categorical features (or those with <20 unique values)\n",
    "# --- ESTA É A PARTE QUE TINHA SIDO OMITIDA ---\n",
    "for col in candidate_features:\n",
    "    if df3[col].dtype == 'object' or df3[col].nunique() < 20:\n",
    "        print(f\"\\n--- {col} unique values ---\")\n",
    "        print(df3[col].fillna('NA').value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca9683-185f-4d9b-94b6-4c92927dd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Preprocessing Pipeline (A Versão \"Corrigida\" do TypeError)\n",
    "# Esta é a célula que falhou no teu notebook 1 original.\n",
    "# Vamos usar a versão CORRIGIDA que descobrimos, que inclui o FunctionTransformer.\n",
    "\n",
    "print(\"A definir o preprocessor (com o fix .astype(str))...\")\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "num_features = [c for c in candidate_features if df3[c].dtype in ['int64','float64'] and c != 'treatment']\n",
    "cat_features = [c for c in candidate_features if c not in num_features]\n",
    "\n",
    "print(\"Numerical features:\", num_features)\n",
    "print(\"Categorical features:\", cat_features)\n",
    "\n",
    "# Numerical pipeline: impute missing values with median, then scale\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: (COM A CORREÇÃO DO TypeError)\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('to_string', FunctionTransformer(lambda x: x.astype(str))), # <-- O FIX MÁGICO\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "], remainder='drop', sparse_threshold=0)\n",
    "\n",
    "print(\"Preprocessor (corrigido) criado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b5679-1052-4583-845d-65771f25f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prepare X,y and separate training/test (A Versão \"Corrigida\" do ValueError)\n",
    "# Esta é a outra célula que falhou. Vamos usar a versão ROBUSTA.\n",
    "\n",
    "print(\"A preparar X e y (com o fix pd.to_numeric)...\")\n",
    "X = df3[candidate_features].copy()\n",
    "\n",
    "# --- CORREÇÃO (Lógica 'pd.to_numeric' Robusta) ---\n",
    "# A lógica .map() antiga falhava e criava 100% de NaNs.\n",
    "y_temp = df3['treatment'].replace({'Yes': 1, 'No': 0})\n",
    "y = pd.to_numeric(y_temp, errors='coerce')\n",
    "# --- FIM DA CORREÇÃO ---\n",
    "\n",
    "# Remove rows with missing target values\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask].astype(int) # .astype(int) agora funciona\n",
    "\n",
    "print(f\"Shape after cleaning NaNs from target: X={X.shape}, y={y.shape}\")\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Train target dist:\", np.bincount(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99debd-6295-4bc0-a146-f801ef3403c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Preprocess Data and Define NN \"Raw\" Functions\n",
    "\n",
    "# 1. Aplicar o preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# 2. Reformular os dados para NumPy (como na tua aula)\n",
    "# O Keras quer (n_amostras, n_features). O NumPy \"raw\" quer (n_features, n_amostras).\n",
    "# Vamos \"transpor\" (Transpose) os dados.\n",
    "X_train_raw = X_train_processed.T\n",
    "X_test_raw = X_test_processed.T\n",
    "\n",
    "# O 'y' tem de ser um \"array\" de 1 linha\n",
    "y_train_raw = y_train.values.reshape(1, y_train.shape[0])\n",
    "y_test_raw = y_test.values.reshape(1, y_test.shape[0])\n",
    "\n",
    "print(f\"Formato 'Raw' de X_train (features, amostras): {X_train_raw.shape}\")\n",
    "print(f\"Formato 'Raw' de y_train (1, amostras): {y_train_raw.shape}\")\n",
    "\n",
    "# 3. Funções \"Helper\" (baseadas na Aula L2)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Calcula a função sigmóide (logística) [cite: 325]\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    \"\"\"Calcula a derivada da sigmóide (para a retropropagação) [cite: 496]\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0452911-2e5b-4dbf-b92f-e92efbebf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Initialize Parameters\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Inicializa os pesos (Thetas) com valores aleatórios pequenos.\n",
    "    n_x: neurónios na camada de input (as nossas features)\n",
    "    n_h: neurónios na camada oculta\n",
    "    n_y: neurónios na camada de output (para nós, é 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(42) # Para reprodutibilidade\n",
    "    \n",
    "    # W1 é o nosso Theta_1 (camada 1 -> 2)\n",
    "    # W2 é o nosso Theta_2 (camada 2 -> 3)\n",
    "    # \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01 # Quebra a simetria [cite: 534]\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6d9ed-99dc-4275-b9b8-9c545ed50b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Forward Propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Executa o \"forward pass\" através da rede. [cite: 451]\n",
    "    \"\"\"\n",
    "    # Retira os pesos da \"gaveta\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Camada 1 -> Camada 2 (Oculta)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1) # Usar 'tanh' como ativação (uma opção da Aula L2) [cite: 326]\n",
    "    \n",
    "    # Camada 2 -> Camada 3 (Output)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2) # Usar 'sigmoid' na saída para classificação binária [cite: 325]\n",
    "    \n",
    "    # Guarda os valores intermédios (cache) para usar na retropropagação\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b89905-111c-4aba-93b2-36c52f32757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Compute Cost Function (with Regularization)\n",
    "def compute_cost(A2, Y, parameters, lambda_reg):\n",
    "    \"\"\"\n",
    "    Calcula o custo (Binary Cross-Entropy) + Regularização L2. [cite: 382, 401]\n",
    "    A2: As nossas previsões (saída da sigmoid)\n",
    "    Y: As respostas corretas (0 ou 1)\n",
    "    lambda_reg: O \"lambda\" da regularização\n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # Número de exemplos\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # Custo de Cross-Entropy [cite: 380]\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
    "    cross_entropy_cost = - (1 / m) * np.sum(logprobs)\n",
    "    \n",
    "    # Custo da Regularização L2 [cite: 401]\n",
    "    L2_cost = (lambda_reg / (2 * m)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    \n",
    "    total_cost = cross_entropy_cost + L2_cost\n",
    "    \n",
    "    return np.squeeze(total_cost) # Garante que é só um número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2f28-e912-4c9f-af8d-3075623d8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Backward Propagation\n",
    "def backward_propagation(parameters, cache, X, Y, lambda_reg):\n",
    "    \"\"\"\n",
    "    Implementa o algoritmo de Error Backpropagation. [cite: 474]\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    Z1 = cache['Z1']\n",
    "    \n",
    "    # 1. Erro na Camada de Saída (delta 3) [cite: 479]\n",
    "    dZ2 = A2 - Y\n",
    "    \n",
    "    # 2. Gradientes da Camada 2-3\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T) + (lambda_reg / m) * W2 # Adiciona grad. da regularização\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # 3. Retropropagar o Erro (delta 2) [cite: 482]\n",
    "    # (Usamos a derivada da 'tanh', que é 1 - A1^2)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    \n",
    "    # 4. Gradientes da Camada 1-2\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T) + (lambda_reg / m) * W1 # Adiciona grad. da regularização\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1d2f3-de24-4c7c-a4ac-5e5fbb223f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Update Parameters (Gradient Descent)\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Atualiza os pesos usando os gradientes. [cite: 486]\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # A regra de atualização\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3921c4-ab74-42af-a8ea-f371ae8c3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: The Full Neural Network Model\n",
    "def nn_model(X, Y, n_h, num_epochs, lambda_reg, learning_rate, print_cost=False):\n",
    "    \"\"\"\n",
    "    Constrói e treina o modelo de NN \"raw\".\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # n_features\n",
    "    n_y = Y.shape[0] # n_outputs (1)\n",
    "    costs = []\n",
    "    \n",
    "    # 1. Inicializar parâmetros\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # 2. Loop de Treino (Epochs) \n",
    "    for i in range(0, num_epochs):\n",
    "        \n",
    "        # 3. Forward propagation [cite: 437]\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # 4. Calcular Custo [cite: 378]\n",
    "        cost = compute_cost(A2, Y, parameters, lambda_reg)\n",
    "        \n",
    "        # 5. Backward propagation [cite: 454]\n",
    "        grads = backward_propagation(parameters, cache, X, Y, lambda_reg)\n",
    "        \n",
    "        # 6. Atualizar parâmetros (Gradient Descent) [cite: 507]\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(f\"Custo após epoch {i}: {cost}\")\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs\n",
    "\n",
    "# Função para fazer previsões\n",
    "def predict(X, parameters):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5).astype(int) # Arredonda para 0 ou 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c223c78-6afd-48da-aa90-8babcdeede19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Train the \"Raw\" MLP\n",
    "\n",
    "# --- Hiperparâmetros ---\n",
    "n_h = 8                # 8 neurónios na camada oculta\n",
    "num_epochs = 2000      # Número de \"rondas\" de treino \n",
    "lambda_reg = 0.1       # Força da regularização (o nosso 'lambda') [cite: 394]\n",
    "learning_rate = 0.01   # Taxa de aprendizagem (o nosso 'alpha') [cite: 508]\n",
    "\n",
    "print(\"A iniciar treino do modelo 'raw'...\")\n",
    "start_time = time.time()\n",
    "\n",
    "parameters, costs = nn_model(\n",
    "    X_train_raw, y_train_raw, \n",
    "    n_h=n_h, \n",
    "    num_epochs=num_epochs, \n",
    "    lambda_reg=lambda_reg, \n",
    "    learning_rate=learning_rate, \n",
    "    print_cost=True\n",
    ")\n",
    "\n",
    "print(f\"Treino 'raw' demorou {time.time() - start_time:.2f} segundos.\")\n",
    "\n",
    "# Plotar a curva de custo (para ver se aprendeu)\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('Custo (Loss)')\n",
    "plt.xlabel('Epochs (x100)')\n",
    "plt.title(f\"Curva de Aprendizagem (Learning Rate = {learning_rate})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea029e-f661-439f-9a91-dfc44b45d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Evaluate the \"Raw\" MLP\n",
    "\n",
    "# 1. Fazer previsões no set de TESTE\n",
    "y_pred_raw_test = predict(X_test_raw, parameters)\n",
    "\n",
    "# 2. Fazer previsões no set de TREINO (para ver se há overfitting)\n",
    "y_pred_raw_train = predict(X_train_raw, parameters)\n",
    "\n",
    "# 3. O y_pred_raw é (1, n_amostras). O y_test é (n_amostras,).\n",
    "# Temos de \"achatar\" (flatten) as previsões para os comparar.\n",
    "y_pred_flat_test = y_pred_raw_test.flatten()\n",
    "y_pred_flat_train = y_pred_raw_train.flatten()\n",
    "\n",
    "\n",
    "# [cite_start]--- Diagnosticar Bias vs. Variance (Aula L3) [cite: 848] ---\n",
    "print(\"\\n--- Diagnóstico de Performance (Aula L3) ---\")\n",
    "print(\"Relatório de Classificação (TREINO):\")\n",
    "print(classification_report(y_train, y_pred_flat_train, digits=4))\n",
    "print(\"---\")\n",
    "print(\"Relatório de Classificação (TESTE):\")\n",
    "\n",
    "# --- A CORREÇÃO ESTÁ AQUI ---\n",
    "# Trocámos y_pred_flat_TRAIN por y_pred_flat_TEST\n",
    "print(classification_report(y_test, y_pred_flat_test, digits=4)) \n",
    "# --- FIM DA CORREÇÃO ---\n",
    "\n",
    "# [cite_start]Se o F1 de Treino for 0.99 e o de Teste 0.80 -> High Variance (Overfitting) [cite: 852]\n",
    "# [cite_start]Se o F1 de Treino for 0.70 e o de Teste 0.69 -> High Bias (Underfitting) [cite: 850]\n",
    "\n",
    "# [cite_start]--- Matriz de Confusão (Aula L3) [cite: 554] ---\n",
    "print(\"\\nMatriz de Confusão (TESTE):\")\n",
    "cm = confusion_matrix(y_test, y_pred_flat_test) # Usa a variável correta aqui também\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp.ax_.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd28ef-56f3-4b08-b15b-35ba20dcd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: O Modelo \"Refinado\" (com Early Stopping)\n",
    "\n",
    "# Esta é uma CÓPIA da 'nn_model' (Célula 14),\n",
    "# mas modificada para incluir Early Stopping.\n",
    "def nn_model_refined(X_train, Y_train, X_val, Y_val, n_h, num_epochs, lambda_reg, learning_rate, patience=20):\n",
    "    \"\"\"\n",
    "    Constrói e treina o modelo \"raw\" com Early Stopping.\n",
    "    X_val, Y_val: Dados de validação para verificar o overfitting\n",
    "    patience: Quantas epochs esperar sem melhoria\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X_train.shape[0]\n",
    "    n_y = Y_train.shape[0]\n",
    "    costs_train = []\n",
    "    costs_val = [] # Guardar o custo de validação\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    best_cost = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for i in range(0, num_epochs):\n",
    "        \n",
    "        # --- Treino ---\n",
    "        A2_train, cache_train = forward_propagation(X_train, parameters)\n",
    "        cost_train = compute_cost(A2_train, Y_train, parameters, lambda_reg)\n",
    "        grads = backward_propagation(parameters, cache_train, X_train, Y_train, lambda_reg)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # --- Validação ---\n",
    "        A2_val, _ = forward_propagation(X_val, parameters)\n",
    "        cost_val = compute_cost(A2_val, Y_val, parameters, lambda_reg)\n",
    "        \n",
    "        costs_train.append(cost_train)\n",
    "        costs_val.append(cost_val)\n",
    "\n",
    "        # --- Lógica de Early Stopping (Sugestão 5) ---\n",
    "        if cost_val < best_cost:\n",
    "            best_cost = cost_val\n",
    "            best_params = parameters.copy() # Guardar os melhores pesos\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"--- Early stopping na epoch {i} ---\")\n",
    "            break\n",
    "            \n",
    "    return best_params, costs_train, costs_val\n",
    "\n",
    "print(\"Função 'nn_model_refined' (com Early Stopping) definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762a49d-8771-4639-8c4b-88e04624b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Grid Search Manual (Sugestão 4)\n",
    "\n",
    "print(\"A iniciar Grid Search manual para o modelo refinado...\")\n",
    "\n",
    "# 1. Definir a Grelha\n",
    "# (É uma grelha mais pequena para ser rápida. Podes aumentar mais tarde)\n",
    "param_grid = {\n",
    "    'n_h': [8, 16], # Neurónios na camada oculta\n",
    "    'lambda_reg': [0.1, 0.5, 1.0], # Força da regularização\n",
    "    'learning_rate': [0.01] # Taxa de aprendizagem\n",
    "}\n",
    "\n",
    "best_score_f1 = -1\n",
    "best_hyperparams = {}\n",
    "best_model_params = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 2. Loop de Grid Search\n",
    "for n_h in param_grid['n_h']:\n",
    "    for lr in param_grid['learning_rate']:\n",
    "        for lam in param_grid['lambda_reg']:\n",
    "            \n",
    "            print(f\"\\nA testar: n_h={n_h}, lr={lr}, lambda={lam}\")\n",
    "            \n",
    "            # Treinar o modelo refinado\n",
    "            # (Usamos X_test_raw/y_test_raw como set de validação)\n",
    "            params, _, _ = nn_model_refined(\n",
    "                X_train_raw, y_train_raw,\n",
    "                X_test_raw, y_test_raw, # Usa o set de teste como validação\n",
    "                n_h=n_h, \n",
    "                num_epochs=2000, # Epochs altas (o Early Stopping trata de parar)\n",
    "                lambda_reg=lam, \n",
    "                learning_rate=lr,\n",
    "                patience=50 # Paciência de 50 epochs\n",
    "            )\n",
    "            \n",
    "            # 3. Avaliar\n",
    "            # (Temos de \"achatar\" as previsões, como na Célula 16)\n",
    "            y_pred_val = predict(X_test_raw, params).flatten()\n",
    "            \n",
    "            # Usar o f1-score (como no Random Forest)\n",
    "            score = f1_score(y_test, y_pred_val) \n",
    "            \n",
    "            if score > best_score_f1:\n",
    "                best_score_f1 = score\n",
    "                best_hyperparams = {'n_h': n_h, 'lr': lr, 'lambda': lam}\n",
    "                best_model_params = params # Salva os pesos (Thetas) do melhor modelo\n",
    "\n",
    "print(f\"\\nGrid Search demorou {time.time() - start_time:.2f} segundos.\")\n",
    "print(f\"\\n--- MELHOR MODELO (REFINADO) ---\")\n",
    "print(f\"Melhor F1-score (na validação): {best_score_f1:.4f}\")\n",
    "print(f\"Melhores Hiperparâmetros: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410d7b3-65a5-424a-977b-bfbd70a8502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Avaliação Final do Modelo Refinado (Sugestões 6 e 7)\n",
    "\n",
    "# 1. Fazer previsões no set de TESTE com o MELHOR modelo\n",
    "y_pred_refined_test = predict(X_test_raw, best_model_params).flatten()\n",
    "y_pred_refined_train = predict(X_train_raw, best_model_params).flatten()\n",
    "\n",
    "print(\"\\n--- Diagnóstico de Performance (MODELO REFINADO) ---\")\n",
    "print(\"Relatório de Classificação (TREINO - Refinado):\")\n",
    "print(classification_report(y_train, y_pred_refined_train, digits=4))\n",
    "print(\"---\")\n",
    "print(\"Relatório de Classificação (TESTE - Refinado):\")\n",
    "print(classification_report(y_test, y_pred_refined_test, digits=4))\n",
    "print(\"---\")\n",
    "\n",
    "# 2. Matriz de Confusão\n",
    "print(\"\\nMatriz de Confusão (TESTE - Refinado):\")\n",
    "cm = confusion_matrix(y_test, y_pred_refined_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "disp.plot(cmap=plt.cm.Blues, colorbar=False)\n",
    "disp.ax_.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# 3. Visualização (Sugestão 6): Distribuição de Probabilidades\n",
    "print(\"\\nDistribuição das Probabilidades Previstas (TESTE - Refinado):\")\n",
    "# (Temos de correr o forward_propagation para ir buscar as probabilidades 'A2')\n",
    "probs_refined, _ = forward_propagation(X_test_raw, best_model_params)\n",
    "probs_flat = probs_refined.flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(probs_flat[y_test == 0], bins=30, alpha=0.5, label='Classe 0 (No)')\n",
    "plt.hist(probs_flat[y_test == 1], bins=30, alpha=0.5, label='Classe 1 (Yes)')\n",
    "plt.xlabel('Probabilidade Prevista de ser \"Yes\"')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Distribuição de Probabilidades (Modelo Refinado)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 4. Análise de Erros (Sugestão 7)\n",
    "print(\"\\n--- Análise de Erros (TESTE - Refinado) ---\")\n",
    "misclassified_idx = np.where(y_test != y_pred_refined_test)[0]\n",
    "print(f\"Total de erros: {len(misclassified_idx)} de {len(y_test)}\")\n",
    "print(\"A mostrar os 5 primeiros erros:\")\n",
    "\n",
    "for idx in misclassified_idx[:5]:\n",
    "    original_idx = y_test.index[idx] # Pega no índice original do DataFrame\n",
    "    true_label = y_test.iloc[idx]\n",
    "    pred_label = y_pred_refined_test[idx]\n",
    "    \n",
    "    print(f\"\\n* Índice (original): {original_idx}\")\n",
    "    print(f\"  Verdade: {true_label}, Previsão: {pred_label}\")\n",
    "    # O 'X' (DataFrame original) tem o índice correto\n",
    "    print(f\"  Features: {X.loc[original_idx].to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8c6cf-f86f-4857-b214-7d5407d34c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Análise de Importância da Feature (Modelo REFINADO)\n",
    "\n",
    "print(\"--- Análise de Importância (Pesos da Camada 1 do Modelo REFINADO) ---\")\n",
    "\n",
    "# --- A CORREÇÃO ESTÁ AQUI ---\n",
    "# 1. Ir buscar os nomes das features (em vez de depender de outra célula)\n",
    "# (Estas variáveis 'preprocessor', 'num_features', 'cat_features' vêm da Célula 6)\n",
    "print(\"A ir buscar os nomes das features do preprocessor...\")\n",
    "onehot_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_features)\n",
    "all_features = num_features + list(onehot_features)\n",
    "print(f\"Encontradas {len(all_features)} features no total.\")\n",
    "# --- FIM DA CORREÇÃO ---\n",
    "\n",
    "# 2. Ir buscar a matriz de pesos W1 (Theta 1) do *melhor* modelo\n",
    "# (best_model_params foi criado na Célula 18)\n",
    "W1_refined = best_model_params['W1'] \n",
    "\n",
    "# 3. Calcular a importância\n",
    "feature_importance_refined = np.mean(np.abs(W1_refined), axis=0)\n",
    "\n",
    "# 4. Criar um DataFrame bonito para ver\n",
    "df_importance_refined = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Importancia (Media Abs W1)': feature_importance_refined\n",
    "})\n",
    "\n",
    "# 5. Mostrar o Top 10\n",
    "print(\"\\nTop 10 Features (baseado na magnitude média dos pesos da 1ª camada):\")\n",
    "display(df_importance_refined.sort_values(by='Importancia (Media Abs W1)', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f701927-68c8-4181-92d1-d73c31c563ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
